# LVIS-INSTRUCT4V

## Introduction

We introduce a fine-grained visual instruction dataset, LVIS-INSTRUCT4V-220K, which contains *220K* visually aligned and context-aware instructions produced by prompting the powerful GPT-4V with images from LVIS. 

## Usage

Please follow [LLaVA](https://github.com/haotian-liu/LLaVA) to set up the code. 

Our data is available at [LVIS-INSTRUCT4V](https://huggingface.co/datasets/X2FD/LVIS-Instruct4V), and we also provide the pretrained checkpoints, including:

- LLaVA-1.5 pretrained on [LVIS-INSTRUCT4V-220K](https://huggingface.co/X2FD/LVIS-Instruct4v-7b)
- LLaVA-1.5 pretrained on [Mixed LVIS-INSTRUCT4V-220K and LLaVA-INSTRUCT-150K](https://huggingface.co/X2FD/LVIS-Instruct4v-LLaVA-7b)

## Acknowledgement

We thank the authors of [LLaVA](https://github.com/haotian-liu/LLaVA) for their contribution to the open-source community.
